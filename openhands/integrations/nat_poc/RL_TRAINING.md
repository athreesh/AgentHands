# RL Training Guide: Making AgentHands Expert at NAT Creation

This guide walks you through training AgentHands to become a specialist at creating complete NAT agents.

## Overview

**Goal**: Use SkyRL to RL-train AgentHands (a coding agent) to excel at:
1. Generating NAT-compliant tools
2. Integrating tools into complete agents (YAML, MCP setup)
3. Validating end-to-end workflows

**Approach**:
- Gemini creates training tasks (cheap, stays unchanged)
- AgentHands attempts to create complete NAT agents
- SkyRL calculates rewards and updates the policy
- AgentHands gets better over time

## Prerequisites

### 1. Hardware Requirements

**Minimum** (small-scale training):
- 1x node with 8x A100/H100 GPUs (80GB)
- 500GB+ disk space
- Good network connection

**Recommended** (full training):
- 2x nodes with 8x H100 GPUs each
- 1TB+ disk space
- Kubernetes cluster (optional, for scaling)

### 2. Software Requirements

```bash
# Clone repositories
cd ~/Desktop/repos

# AgentHands (already cloned)
cd AgentHands
pip install -e .

# SkyRL
git clone --recurse-submodules https://github.com/NovaSky-AI/SkyRL
cd SkyRL/skyrl-train
uv sync --extra vllm
source .venv/bin/activate

# SkyRL-Gym (for environment)
cd ../skyrl-gym
pip install -e .
```

### 3. API Keys

```bash
# Gemini (for dataset preparation)
export GEMINI_API_KEY=your_gemini_api_key

# WandB (for experiment tracking)
export WANDB_API_KEY=your_wandb_api_key
```

## Step 1: Prepare Training Dataset

The dataset consists of NAT agent specifications generated by Gemini.

### Create Dataset Preparation Script

```bash
cd ~/Desktop/repos/AgentHands/openhands/integrations/nat_poc
mkdir -p skyrl_integration
```

Create `skyrl_integration/prepare_dataset.py`:

```python
"""
Prepare RL training dataset using Gemini planner.

This script:
1. Takes user requests (from examples + synthetic generation)
2. Uses Gemini to create agent plans
3. Saves as parquet files for SkyRL training
"""

import asyncio
import pandas as pd
from pathlib import Path
import sys
import os

# Add AgentHands to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))

from openhands.integrations.nat_poc.gemini_planner import GeminiPlanner

async def create_training_dataset(output_dir: Path, num_samples: int = 1000):
    """Create dataset of NAT agent specs for RL training"""

    planner = GeminiPlanner(api_key=os.getenv("GEMINI_API_KEY"))

    # Load or generate user requests
    user_requests = load_user_requests(num_samples)

    agent_specs = []

    for i, request in enumerate(user_requests):
        print(f"Processing {i+1}/{len(user_requests)}: {request[:60]}...")

        try:
            # Gemini creates the plan
            plan = await planner.analyze_intent(request)

            # Create agent spec for RL training
            spec = {
                "task_id": f"nat_agent_{i:04d}",
                "user_request": request,
                "plan": {
                    "scaffold_type": plan.scaffold_type,
                    "mcp_servers": [
                        {
                            "name": s.name,
                            "smithery_id": s.smithery_id,
                            "capabilities_used": s.capabilities if hasattr(s, 'capabilities') else [],
                            "reasoning": s.reasoning if hasattr(s, 'reasoning') else ""
                        }
                        for s in (plan.mcp_servers if hasattr(plan, 'mcp_servers') else [])
                    ],
                    "custom_tools": [
                        {
                            "name": t.name,
                            "purpose": t.purpose,
                            "input_schema": t.input_schema,
                            "output_schema": t.output_schema,
                            "dependencies": t.dependencies if hasattr(t, 'dependencies') else [],
                        }
                        for t in (plan.missing_tools if hasattr(plan, 'missing_tools') else [])
                    ]
                },
                "test_cases": [
                    {
                        "test_id": f"tc_{j}",
                        "query": tc.input_query if hasattr(tc, 'input_query') else tc.get('query', ''),
                        "expected_behavior": tc.expected_behavior if hasattr(tc, 'expected_behavior') else tc.get('expected', '')
                    }
                    for j, tc in enumerate((plan.test_cases if hasattr(plan, 'test_cases') else []))
                ]
            }

            agent_specs.append(spec)

        except Exception as e:
            print(f"  Error: {e}")
            continue

    # Train/val split (90/10)
    split = int(0.9 * len(agent_specs))
    train_df = pd.DataFrame(agent_specs[:split])
    val_df = pd.DataFrame(agent_specs[split:])

    # Save as parquet
    output_dir.mkdir(parents=True, exist_ok=True)
    train_df.to_parquet(output_dir / "train.parquet")
    val_df.to_parquet(output_dir / "val.parquet")

    print(f"\n✅ Dataset created:")
    print(f"   Train: {len(train_df)} examples")
    print(f"   Val: {len(val_df)} examples")
    print(f"   Saved to: {output_dir}")

def load_user_requests(num_samples: int):
    """Load user requests from examples and generate synthetic ones"""

    # Start with real examples
    examples = [
        "I want a calculator that can also tell me the current weather",
        "Create an agent that can research stocks and analyze financial statements",
        "Build an agent for web research that can search and summarize articles",
        "I need an agent to help with SQL queries on my database",
        "Create a coding assistant that can write and test Python functions",
        "Build an agent that can monitor GitHub repos and summarize PRs",
        "I want an agent for customer support that can search knowledge base",
        "Create an agent that can analyze CSV data and generate insights",
        "Build an agent for travel planning that can search flights and hotels",
        "I need an agent to help with academic research and paper summaries",
        # Add more examples...
    ]

    # Generate more synthetic requests using GPT-4 if needed
    # For now, cycle through examples
    requests = examples * (num_samples // len(examples) + 1)
    return requests[:num_samples]

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir", default="~/data/nat_agent_specs", help="Output directory")
    parser.add_argument("--num_samples", type=int, default=1000, help="Number of samples")
    args = parser.parse_args()

    output_dir = Path(args.output_dir).expanduser()
    asyncio.run(create_training_dataset(output_dir, args.num_samples))
```

### Run Dataset Preparation

```bash
# Create training dataset
python skyrl_integration/prepare_dataset.py \
  --output_dir ~/data/nat_agent_specs \
  --num_samples 1000

# This will create:
#   ~/data/nat_agent_specs/train.parquet (900 examples)
#   ~/data/nat_agent_specs/val.parquet (100 examples)
```

## Step 2: Create SkyRL Environment

The environment defines how AgentHands interacts with NAT agent creation tasks.

Create `skyrl_integration/nat_agent_env.py`:

(See complete implementation in the design above - I'll create a condensed version here)

```python
"""
SkyRL Environment for NAT Agent Creation

This environment trains AgentHands to create complete NAT agents.
"""

from skyrl_gym.envs.base_text_env import BaseTextEnv, BaseTextEnvStepOutput, ConversationType
from typing import Any, Dict, Tuple
from omegaconf import DictConfig
from pathlib import Path
import subprocess
import yaml
import json

class NATAgentCreationEnv(BaseTextEnv):
    """
    Environment for RL-training AgentHands on NAT agent creation

    Episode = Create one complete NAT agent
    Reward = Based on tools + integration + workflow success
    """

    def __init__(self, env_config: DictConfig, extras: Dict[str, Any] = {}):
        super().__init__()

        self.agent_spec = extras["agent_spec"]
        self.max_turns = env_config.get("max_turns", 30)

        # Workspace setup
        self.agent_id = self.agent_spec["task_id"]
        self.workspace_dir = Path(env_config.workspace_dir) / self.agent_id
        self.workspace_dir.mkdir(parents=True, exist_ok=True)

        self.nat_tools_dir = self.workspace_dir / "nat_tools"
        self.tests_dir = self.workspace_dir / "tests"
        self.agent_config_file = self.workspace_dir / "agent_config.yml"
        self.mcp_setup_script = self.workspace_dir / "setup_mcp.sh"

        self.chat_history: ConversationType = []

    def init(self, prompt: ConversationType) -> Tuple[ConversationType, Dict[str, Any]]:
        """Initialize episode with agent creation task"""
        initial_prompt = self._create_agenthands_prompt()
        return initial_prompt, {"agent_id": self.agent_id}

    def _create_agenthands_prompt(self) -> ConversationType:
        """Create prompt for AgentHands agent"""
        # (See full implementation in design above)
        pass

    def step(self, action: str) -> BaseTextEnvStepOutput:
        """Process AgentHands action"""
        self.turns += 1

        is_done = "AGENT_CREATION_COMPLETE" in action or self.turns >= self.max_turns

        if is_done:
            validation = self._validate_complete_agent()
            reward = self._calculate_reward(validation)

            return BaseTextEnvStepOutput(
                observations=[],
                reward=reward,
                done=True,
                metadata={"validation": validation, "success": validation["complete"]}
            )

        return BaseTextEnvStepOutput(
            observations=[],
            reward=0.0,
            done=False,
            metadata={"turn": self.turns}
        )

    def _validate_complete_agent(self) -> Dict[str, Any]:
        """Validate tools, integration, and workflow"""
        # (See full implementation in design above)
        pass

    def _calculate_reward(self, validation: Dict[str, Any]) -> float:
        """
        Reward = 0.3*tools + 0.3*integration + 0.4*workflow + bonuses
        """
        # (See full implementation in design above)
        pass
```

### Register Environment with SkyRL

Add to SkyRL's environment registry:

```bash
# Copy environment to SkyRL
cd ~/Desktop/repos/SkyRL/skyrl-gym
cp ~/Desktop/repos/AgentHands/openhands/integrations/nat_poc/skyrl_integration/nat_agent_env.py \
   skyrl_gym/envs/nat_agent_creation.py

# Register in skyrl_gym/envs/__init__.py
# Add: from skyrl_gym.envs.nat_agent_creation import NATAgentCreationEnv
```

## Step 3: Configure Training

Create training configuration:

```bash
cd ~/Desktop/repos/SkyRL/skyrl-train
mkdir -p examples/nat_agent_creation
```

Create `examples/nat_agent_creation/train_agenthands.sh`:

```bash
#!/bin/bash
set -x

# RL Training for AgentHands on NAT Agent Creation

export DATA_DIR="$HOME/data/nat_agent_specs"
export NUM_GPUS=8
export LOGGER=wandb

# Base LLM for AgentHands
export MODEL="Qwen/Qwen2.5-Coder-32B-Instruct"

uv run --isolated --extra vllm -m skyrl_train.entrypoints.main_base \
  data.train_data="['$DATA_DIR/train.parquet']" \
  data.val_data="['$DATA_DIR/val.parquet']" \
  \
  `# RL Algorithm` \
  trainer.algorithm.advantage_estimator="grpo" \
  \
  `# Model` \
  trainer.policy.model.path="$MODEL" \
  \
  `# Distributed Training` \
  trainer.placement.colocate_all=false \
  trainer.strategy=fsdp2 \
  trainer.placement.policy_num_gpus_per_node=$NUM_GPUS \
  trainer.placement.critic_num_gpus_per_node=$NUM_GPUS \
  trainer.placement.ref_num_gpus_per_node=$NUM_GPUS \
  \
  `# Generation (vLLM inference)` \
  generator.num_inference_engines=$NUM_GPUS \
  generator.backend=vllm \
  generator.async_engine=true \
  generator.batched=true \
  generator.n_samples_per_prompt=4 \
  generator.sampling_params.max_generate_length=8192 \
  \
  `# Training Hyperparameters` \
  trainer.epochs=100 \
  trainer.eval_batch_size=32 \
  trainer.train_batch_size=128 \
  trainer.policy_mini_batch_size=32 \
  trainer.micro_forward_batch_size_per_gpu=4 \
  trainer.micro_train_batch_size_per_gpu=4 \
  trainer.policy.optimizer_config.lr=5.0e-7 \
  trainer.algorithm.use_kl_loss=true \
  \
  `# Environment` \
  environment.env_class=nat_agent_creation \
  environment.env_config.workspace_dir="/tmp/nat_workspaces" \
  environment.env_config.max_turns=30 \
  \
  `# Checkpointing` \
  trainer.ckpt_interval=10 \
  trainer.ckpt_path="$HOME/ckpts/agenthands_nat" \
  \
  `# Logging` \
  trainer.logger="$LOGGER" \
  trainer.project_name="agenthands_nat_rl" \
  trainer.run_name="qwen_coder_32b_nat_specialist" \
  \
  $@
```

## Step 4: Launch Training

```bash
# Configure Ray for uv
export RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook

# Launch training
cd ~/Desktop/repos/SkyRL/skyrl-train
bash examples/nat_agent_creation/train_agenthands.sh
```

### Monitor Training

```bash
# Check WandB dashboard
# https://wandb.ai/your-username/agenthands_nat_rl

# Or tail logs
tail -f /tmp/ray/session_latest/logs/*
```

### Expected Progress

```
Epoch 1:   Mean reward: 0.3  Success rate: 25%
Epoch 10:  Mean reward: 0.5  Success rate: 40%
Epoch 25:  Mean reward: 0.8  Success rate: 55%
Epoch 50:  Mean reward: 1.1  Success rate: 70%
Epoch 100: Mean reward: 1.4  Success rate: 85% ✅
```

## Step 5: Evaluate Trained Model

```bash
# Load trained checkpoint
CKPT_PATH="$HOME/ckpts/agenthands_nat/checkpoint_epoch_100"

# Test on validation set
cd ~/Desktop/repos/AgentHands
python -m openhands.integrations.nat_poc.evaluate_trained_model \
  --checkpoint $CKPT_PATH \
  --test_data ~/data/nat_agent_specs/val.parquet \
  --num_samples 20
```

## Step 6: Deploy Trained AgentHands

```python
# Use trained model in production

from openhands.integrations.nat_poc.gemini_planner import GeminiPlanner
from agenthands_rl import TrainedAgentHands

# Load trained model
agenthands = TrainedAgentHands(
    checkpoint_path="~/ckpts/agenthands_nat/checkpoint_epoch_100"
)

# Gemini creates plan
planner = GeminiPlanner(api_key=GEMINI_API_KEY)
plan = await planner.analyze_intent(user_request)

# Trained AgentHands creates complete agent
agent_workspace = await agenthands.create_nat_agent(
    plan=plan,
    workspace_dir="/tmp/production_agent"
)

# Agent is ready to deploy! ✅
print(f"Agent created at: {agent_workspace}")
```

## Troubleshooting

### Out of Memory

```bash
# Reduce batch sizes
trainer.micro_forward_batch_size_per_gpu=2
trainer.micro_train_batch_size_per_gpu=2
```

### Training Too Slow

```bash
# Use more GPUs
export NUM_GPUS=16  # 2 nodes

# Or reduce samples per prompt
generator.n_samples_per_prompt=2
```

### Low Success Rate

```bash
# Train longer
trainer.epochs=200

# Or use stronger base model
export MODEL="Qwen/Qwen2.5-Coder-70B-Instruct"
```

### Dataset Issues

```bash
# Regenerate with more examples
python skyrl_integration/prepare_dataset.py --num_samples 2000

# Or improve diversity
# Edit prepare_dataset.py to add more varied requests
```

## Advanced Topics

### Curriculum Learning

Start with simple tasks, gradually increase difficulty:

```python
# In prepare_dataset.py
def estimate_difficulty(tool_spec):
    # Simple: 1-2 tools, no MCP
    # Medium: 3-4 tools, 1-2 MCP servers
    # Hard: 5+ tools, 3+ MCP servers
    pass

# Sort dataset by difficulty
agent_specs = sorted(agent_specs, key=lambda x: x["difficulty"])
```

### Multi-Node Training

```bash
# On head node
ray start --head --port=6379

# On worker nodes
ray start --address=<head_node_ip>:6379

# Run training (will use all nodes)
bash examples/nat_agent_creation/train_agenthands.sh
```

### Hyperparameter Tuning

```bash
# Try different learning rates
for lr in 1e-7 5e-7 1e-6; do
  bash train_agenthands.sh trainer.policy.optimizer_config.lr=$lr
done
```

## Next Steps

1. ✅ Train base model (100 epochs)
2. Fine-tune on specific domains (finance, research, etc.)
3. Collect user feedback in production
4. Retrain with human preferences (RLHF)
5. Deploy as API service

## Resources

- SkyRL Docs: https://skyrl.readthedocs.io
- AgentHands Repo: https://github.com/athreesh/AgentHands
- NAT Toolkit: https://github.com/NVIDIA/NeMo-Agent-Toolkit
- Questions: Open an issue in AgentHands repo
